{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598973181530",
   "display_name": "Python 3.8.3 64-bit ('pm3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Model Comparison\n",
    "Todo: Explain what's going on here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic imports and setup.\n",
    "\n",
    "from configparser import ConfigParser\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import arviz\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "from neuropsymodelcomparison.dataprocessing.modelcomparator import ModelComparison\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 120)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "\n",
    "# Default file format for plotly figures.\n",
    "pio.kaleido.scope.default_format = \"pdf\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common folder paths.\n",
    "data_path = Path('../data/preprocessed/')\n",
    "reports_path = Path('../reports/')\n",
    "figures_path = reports_path / 'figures'\n",
    "\n",
    "# Read in data.\n",
    "# We want to use the same threshold for including sessions as we used previously. So all should be included.\n",
    "try:\n",
    "    sample_info = Path.read_text(reports_path / 'sampling.txt')\n",
    "    sample_info = '[dummy_section]\\n'+ sample_info\n",
    "except IOError:\n",
    "    min_trials = 18  # Fallback: 60% of 30 trials.\n",
    "else:\n",
    "    config_parser = ConfigParser()\n",
    "    config_parser.read_string(sample_info)\n",
    "    min_trials = int(config_parser.get('dummy_section', 'trials_count_threshold', fallback=18))\n",
    "\n",
    "trial_data = pd.read_csv(data_path / 'trials.csv', index_col='id')\n",
    "# We only analyze the first session of each participant.\n",
    "df = trial_data.loc[trial_data['session'] == 1, ['user', 'block', 'parallel', 'orthogonal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_comp = ModelComparison(df, min_samples=min_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare the data of each user to our theoretical models.\n",
    "# This will take quite some time!\n",
    "for user in df['user'].unique():\n",
    "    model_comp.compare_models(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior probabilites in the context of condition and gaming experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment posterior data.\n",
    "# Condition.\n",
    "conditions = trial_data.loc[trial_data['user'].isin(model_comp.posteriors.index), ['user','condition']].drop_duplicates().set_index('user')\n",
    "model_comp.posteriors = model_comp.posteriors.join(conditions)\n",
    "# Gaming experience.\n",
    "exp = pd.read_csv('../data/raw/users.csv', dtype={'gaming_exp': pd.Int8Dtype()}).loc[model_comp.posteriors.index, 'gaming_exp']\n",
    "model_comp.posteriors = model_comp.posteriors.join(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_posteriors = px.imshow(model_comp.posteriors.drop(['condition', 'gaming_exp'], axis='columns').reset_index(drop=True), labels=dict(x=\"Model\", y=\"Participant\", color=\"Posterior<br>Probability\"), color_continuous_scale='Greys', zmin=0, zmax=1, aspect='equal', height=len(model_comp.posteriors)*30, width=500)\n",
    "fig_posteriors.update_xaxes(side=\"top\", showspikes=True, spikemode=\"across\")\n",
    "fig_posteriors.update_yaxes(tickmode='array', tickvals=list(range(len(model_comp.posteriors))), ticktext=model_comp.posteriors.index, showspikes=True)\n",
    "fig_posteriors.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save tables.\n",
    "out_file = reports_path / \"posteriors.csv\"\n",
    "model_comp.write_posteriors(out_file)\n",
    "logging.info(f\"Written report to {out_file.resolve()}\")\n",
    "\n",
    "# Save Figures\n",
    "fig_filepath = figures_path / 'heatmap-posteriors.pdf'\n",
    "fig_posteriors.write_image(str(fig_filepath))\n",
    "logging.info(f\"Written figure to {fig_filepath.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}